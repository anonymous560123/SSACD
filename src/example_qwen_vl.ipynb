{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0496e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab7fb400c5d4ca8abf025ae8ce8ea53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This video appears to be a cooking tutorial from the \"Bake Like A Pro\" YouTube channel. \n",
      "\n",
      "In the initial frames, we see a close-up of a person straining a mixture into a bowl. The text overlay states, \"We now want to strain our mixture, to remove most of the moisture.\" This suggests that the person is preparing a mixture for a recipe and removing excess liquid or moisture to achieve the desired consistency.\n",
      "\n",
      "In the subsequent frames, the focus shifts to the preparation of pancakes. The person is seen using kitchen tongs to flip some pancake slices in a pan. There is also a bowl with what seems to be pancake batter next to the pan, which indicates that the pancakes are in the process of cooking.\n",
      "\n",
      "The video likely provides step-by-step instructions on how to make pancakes, with a focus on techniques such as straining the batter and cooking the pancakes properly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "from utils.video_utils import load_video, add_pertubation\n",
    "from utils.genearte_cd import generate_cd\n",
    "import copy\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "clean_images = load_video(\n",
    "    \"/data/kyubin/VidHalluc/eval/inference/VidHalluc/data/STH/_AZ5jo3y7V4_clip_2_5.mp4\",\n",
    "    num_segments=8,\n",
    ")\n",
    "noise_images = copy.deepcopy(clean_images)\n",
    "noise_images, has_candidate = add_pertubation(video_np=noise_images, gaussian_std=75, THRESHOLD=0.8)\n",
    "\n",
    "clean_prompt = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"video\", \"video\": clean_images},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this video\"},\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "noise_prompt = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"video\", \"video\": noise_images},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this video\"},\n",
    "    ]\n",
    "}]\n",
    "\n",
    "clean_inputs = processor.apply_chat_template(\n",
    "    clean_prompt,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    num_frames=8,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "noise_inputs = processor.apply_chat_template(\n",
    "    noise_prompt,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    num_frames=8,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generate_ids = generate_cd(\n",
    "        clean_inputs, noise_inputs, model, processor,\n",
    "        cd_alpha=0, cd_beta=0, use_cd=False\n",
    "    )\n",
    "    output_text = processor.decode(generate_ids[0, clean_inputs[\"input_ids\"].shape[1]:],\n",
    "                                        skip_special_tokens=True)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20020faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
